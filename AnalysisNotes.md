# Soybean looper MIPS analysis

Note that prior to starting analysis, I set up a fresh conda environment (SoybeanLooper) to install needed programs via bioconda wherever possible. Analysis should be run from this environment. Steps in the analysis are recorded, and can be repreated using the Makefil in this directory. The Makefile, these notes and other small text files are kept under a git repository. Large files are not kept in the repository. In particular, fastq files and other output produced by the MiniSeq are in a directory <MiniSeqOutput> that is not tracked by git.

## Generating genomic sequence

To generate some genomic contigs, sequenced a single individual: MA22 with one lane of MiniSeq. Original data are stored in <MiniSeqOutPut/20180331_155401>. To keep things easy to follow, set up a directory <GenomeRawData> and linked the fastq files into it.

### Initial data QC.

Ran fastqc 0.11.7 on the two raw data files. Total number of read pairs is 28,001,390. Overall, the data look decent. Theere is as usual, both a decline in mean quality score and an increase in variance in quality score toward the end of the reads. Fastqc does not find any adaptor sequences. There is one overrepresented sequence in the read 2 data, but not identifiable source, and it is found in 0.19% of reads.

Conclusion - we are in good shape, but we will want to do some quality-trimming before attempting assembly.

### Quality trimming and adaptor clipping

Used trimmomatic to trim low quality sequence. Also ran an adaptor clipping step. No evidence of adaptor contamination, but it can't hurt!

### QC of trimmed data

Re-ran fastqc on the trimmed paired data. Paired reads look good, quality is hig, no overrepresented sequences, 25,075,929 pairs of reads retained.

### Assembly with SPAdes

We expect our genome to be highly heterozygous, which will tend to produce a fragmented assembly with many redundant contigs corresponding to alleles at the same loci. Hugh saw this problem with the WCR genome, and the redundans pipline did a decent job of cleaning up redundant contigs and improving scaffold length. We will use the same approach. In the redundans paper, they tried several DeBruijn assemblers, and SPAdes seemed to perform best, so we will do likewise. Spades puts all of its output into a specified dir, so we can use the output dir as the make target.

#### Results

SPAdes generates both contigs and scaffolds

Some quick summary statistics generated by the Sanger Centre assembly-stats tool to compare contigs and scaffolds:

stats for spades.assembley/contigs.fasta
sum = 395462374, n = 172429, ave = 2293.48, largest = 73087
N50 = 4701, n = 22932
N60 = 3554, n = 32608
N70 = 2551, n = 45737
N80 = 1682, n = 64814
N90 = 1047, n = 95012
N100 = 78, n = 172429
N_count = 0
Gaps = 0

And for scaffolds

stats for spades.assembley/scaffolds.fasta
sum = 395600906, n = 169710, ave = 2331.04, largest = 73087
N50 = 4883, n = 22015
N60 = 3682, n = 31342
N70 = 2622, n = 44068
N80 = 1708, n = 62758
N90 = 1052, n = 92689
N100 = 78, n = 169710
N_count = 138532
Gaps = 2719

Scaffolding has made very little difference with respect to N50 or overall assembly size.

Interestingly, our total assembly size of ~395 Mb is about what we would expect for a complete Lep genome.

A more detailed analysis of the SPAdes scaffols can be found in the R/SPAdesAnalysis.Rmd file

Things I learned from analysing with R:

91% of the assembly is in contigs >= 1kb.

The modal value of coverage per base for contigs is around 5. Fairly low, but apprently sufficient to generate some decent contigs. We do have some contigs with high coverge that are probably repetitive elemements. About 88% of the assembly appears to be in contigs >= 1 kb and with coverages that suggest they are non-repetitve.

All told, I'm pretty happy with that. Nevertheless, it can't hurt to have Redundans try and clean things up a bit.

### Cleanup with redundans

Redundans is not available via bioconda, but it is available as a docker image. Opted to use the docker image because redundans has a fair number of dependencies to install. Ran redundans with the default parameters. Presumably as a result of the way redundans runs in docker, its output directory is owned by root. Had to chown the output dir.

Redundans produces a "reduced" set of contigs, which are then scaffolded and "filled". Assembly stats are

stats for redundans/out/contigs.reduced.fa
sum = 356934037, n = 119040, ave = 2998.44, largest = 73087
N50 = 5323, n = 19081
N60 = 4192, n = 26639
N70 = 3198, n = 36374
N80 = 2278, n = 49554
N90 = 1394, n = 69400
N100 = 200, n = 119040
N_count = 0
Gaps = 0

stats for redundans/out/scaffolds.reduced.fa
sum = 356476318, n = 112608, ave = 3165.64, largest = 77024
N50 = 5756, n = 17615
N60 = 4534, n = 24595
N70 = 3447, n = 33600
N80 = 2452, n = 45819
N90 = 1476, n = 64379
N100 = 200, n = 112608
N_count = 8994
Gaps = 1169

stats for redundans/out/scaffolds.filled.fa
sum = 356588794, n = 112747, ave = 3162.73, largest = 77024
N50 = 5754, n = 17625
N60 = 4531, n = 24610
N70 = 3446, n = 33622
N80 = 2450, n = 45854
N90 = 1475, n = 64440
N100 = 200, n = 112747
N_count = 8994
Gaps = 1169

There seems to be very little difference beween the scaffolds produced by redundans and the filled scaffolds. But there is a fair improvement from the SPAdes contigs to the filled scaffolds.

+------------------------------+------------------------------+------------------------------+
|                              |SPAdes contigs                |Redundans filled scaffolds    |
+------------------------------+------------------------------+------------------------------+
|N50                           |4701                          |5754                          |
+------------------------------+------------------------------+------------------------------+
|Total contigs / scaffolds     |172,429                       |112,747                       |
+------------------------------+------------------------------+------------------------------+
|Total assembly size           |395,462,374                   |356,588,794                   |
+------------------------------+------------------------------+------------------------------+

### Repeats

We don't want to desgn a bunch of MIPs that target repetitive elements. Use RepeatModeler to do de-novo repeat identification and RepeatMasker to mask out any repeats

After running for two days, RepeatModeler finds nothing. Here's what it has to say for itself:

```
Discovery complete: 0 families found
Program Time: 45:20:05 (hh:mm:ss) Elapsed Time
Working directory:  /work/nick/SoybeanLooper/RM_30101.TueMay11117282018
may be deleted unless there were problems with the run.
No families identified.  Perhaps the database is too small
or contains overly fragmented sequences.
```

This might be because repreats are collapsing into a few contigs, so they are only represented once each in the assembly.

If so, we might expect high read coverage in repeat regions, although this might be complicated by the fact that we had a PCR step in our library prep. We can try and solve that by marking / removing likely PCR duplicates with SAMTools or whatever.

Aligned the trimmed/cleaned reads to the redundans filled scaffolds with bwa mem.

Removed duplicates and calculated read depth at each position in the assembly using samtools.

Following some analysis of read depths in R, plus some eyeballing in Tablet, it looks like if we use regions with coverage <= 50 reads, we should be in single-copy regions of the genome.

## Identifying probable single copy regions

We want to identify a set of single copy regions that can be used used for targets for MIPS probes. Basic approach:

 1. Find all sites in the assembly where depth of coverage <= 50
 2. Merge contiguous regions of coverage <= 50
 3. Discard any small contiguous regions < 200bp, as these will not make good targets for MIPs.
 4. Drop any contigs < 1 kb, these only make up a few % of the total assembly. Very small contigs have a higher sisk of being junk.
 
We can use bedtools for most of this. **NB** BED files number positions starting at 0.

Eyeballing a few contigs in Tablet suggests this is pretty conservative. We are probably excluding a fair bit of single copy sequence where coverage exceeds 50 reads. Nevertheless, the bed file of regions with <= 50 reads coverage contains 352,306,659 bases, 98.7% of the redundans scaffolded assembly.

After removal of scaffolds < 1kb, we are left with 335,481,145 bases of acceptable sequence, 94.1% of the redundans scaffolded assembly.

### Identifying MIPs probes with MIPGEN

The MIPGEN tool from the Shendure lab is designed to identify MIPs probes for specified targets. It also checks that probes won't hybridize elsewhere in the genome and calculates a score reflecting the probability that the probe will produce a viable assay. All of this is very helpful. However MIPGEN is intended for designing tiled sets of probes that cover larger features. We don't want to do this, because we are interested in surveying sites scattered throughout the genome. This isn't a big deal as we can just pick one probe from a tiled set.

#### MIPGEN is a little fussy

After playing around with MIPgen, I noticed some things that cause it to fail. This was figured out by trial and error, since the MIPGEN documentation and paper didn't really give me any clues as to what was going on. Known issues include:

 * MIPGen chokes on fasta files that contain pipe characters in the identifier lines. This appears to be because it passes identifiers directly to samtools faidx without quoting the identifier.
 * MIPGen fails on features at the start of a contig. No idea why, but avoiding features < 300 bases from the start seems to serve as a workaround.
 * MIPGen fails at the tiling stage on large features. This seems to be because if it cannot tile across the feature it fails.

To work around these issues the basic approach is

 1. Update the redundans assembly to remove pipe chars from identifiers.
 2. Regenerate BED files to match identifiers.
 3. Generate a new BED file with 1bp targets, evenly spaced through the non-repetitive regions at 500bp intervals. targets within 200 bases of the start or end of the scaffold are dropped. This has the effect that, at the tiling stage, MIPgen only needs to "tile" over a single base, so it just picks the best probe for each 1bp target. This also reduces the sizes of the output files considerably.

These workarounds were figured out in discussion with Evan Boyle and Jay Shendure. Be sure to acknowledge them in the paper.

Addistional conversations with Evan established that the short scaffolds we have were causing a problem. Here's Evan's explanation in full:

*"I was able to recognize what is happening. I didn’t anticipate MIPgen being used on such small scaffolds. By design it acquires sequence from +/- 1kb of the area of interest. I found that this information slightly improved performance.

Right not that sequence doesn’t exist because the scaffold is too small. Fortunately, with the default logistic scoring, this sequence is not needed anyway and you can basically turn this off.

I don’t think I have time to add an option to the code (mostly because I am always afraid of breaking it incidentally) in the next few days, but if you are savvy enough, if you go to lines 1126 and 1127 of the main script (mipgen.cpp) and replace the lines as such (simply deleting the extra 1000 nucleotides):"*
 
```
+ boost::lexical_cast<string>(feature->start_position_flanked - max_capture_size) + "-“ \ + boost::lexical_cast<string>(feature->stop_position_flanked + max_capture_size
 + 14) +
 "
 " \
 ```
 
 *"Then as long as you keep -score_method logistic, I think it should run as you expect and tile those regions"*


### Running MIPgen on a reduced set of targets

After attempting to run MIPgen with all possible targets, it became apparent that doing so would
 
  1. Take several days, possibly more than a week.
  2. Generate huge output files, totalling sevaeral Tb

The solution is to sample from the total set of potential targets. Generating a sample of 10000 targets should still give us plenty of scope to select 50 high-quality MIPs that a re scattered throughout the genome. The R script that does the subsampling accepts a random seed as an argument. This is used to ensure that the same sample of targets can be selected every time the script is run.

Made a subsample of 10000 targets and ran MIPgen. The picked_mips file contains 3224 MIPs with logistic scores >= 0.98 (ie very high scoring), in 3097 different scaffolds. 

### Choosing MIPs to test

Details are documented in the R markdown notebook. Basic approach was

 1. Filter picked MIPs to retain the MIPs with the highest logistic score in each scaffold.
 2. Sort the set of MIPs by logistic score
 3. Select the 50 MIPs with the highest logistic scores
 
## First test run

The first test sequencing run consisted of a total of 64 individauls. Ten were fall armworms, 46 were soybean loopers with 8 soybean loopers run in duplicate. Sequencing was succesful insofar as I got data and the indexing reads split the data into individual samples.

On a side note, I'm trying out snakemake as an alternative to make. It's got a bit of learning curve but it looks like it will work better running jobs in parallel when there are multiple input or output files per job.

### Initial QC

### Trimming

Reads are 151 bases long. Inserts range from 125 - 138 bp, so we definitely expect to see primer sequences showing up at the end of the reads. Used trimmomatic to remove the primer sequences. Because the primers inlude index sequences, set up the file with just the parts of the primers 3' to the indexes.

Confusingly, I don't seem to be finding any primer sequence.

Explanation - I was wrong about the insert size, because it include the extension and ligation arms. All insers are 154 bp, so we do not expect to see any primer.

### Read merging

Reads should almost span the entire insert. We can merge read pairs into singe leads with PEAR. Looking at the fastqc results for the trimmed reads there was a small population of reads with lengths <= 80 bases. Re-ran trimmomatic with minlen increased to 85 to get rid of these

### Alignment

Ran alignment with bwa mem and default parameters. Visualized aligned reads with Tablet. I see three basic patterns.

1. Loci where no reads align
2. Loci where reads align along their full lengths. Eyeballing suggests plenty of heterozygotes
3. Loci where reads only align along part of their length and are "soft clipped"

This third category is a bit of a worry. Some quick eye\balling indicates that the clipped reads match to the target locus at both ends but not in the middle. This indicates that they are off-target reads from genome locations that match the probe arms. This could be a result of repetitive elements, despite our best efforts to avoid those.

In principle, a simple way to deal with this is to not include / get rid of soft-clipped reads. Bwa pretty much insists on soft clipping but bowtie, for example defaults to end-to-end alignments. Using bowtie 2 does indeed get rid of the clipped reads. This results in some additional loci wuth no reads aligning. That's OK, because they were not useable loci in the first place.

A comparison with MIPs data from fall armyworm partially supports the hypothesis that the off-target clipped reads were due to repetitive elements. The FAW MIPs were designed from a published genome sequence with repeats masked. I see a lot fewer loci in FAW with soft-clipped reads when aligning tith bwa. Nevertheless there are still a couple of loci with clipped reads, so repeats may no be the only issue
